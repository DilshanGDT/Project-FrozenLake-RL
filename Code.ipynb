{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bia7kVZMjNlQ"
      },
      "source": [
        "#### The Frozen Lake Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HRkFMITko3_"
      },
      "source": [
        "(a) Create the Frozen Lake environment with a 4×4 grid using ’FrozenLake-v1’ version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "K2Inx1SzjS-2"
      },
      "outputs": [],
      "source": [
        "# Import the neccesary libraries\n",
        "import gym\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5FZyotoakM25"
      },
      "outputs": [],
      "source": [
        "# Create the FrozenLake-v1 environment with a 4x4 grid\n",
        "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True)\n",
        "\n",
        "# Reset the environment to get the initial state\n",
        "state, info = env.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3M9ZUJXNykxJ"
      },
      "source": [
        "- S - Start tile\n",
        "- G - Goal tile\n",
        "- F - frozen tile\n",
        "- H - a tile with a hole"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T046WKbcyOgK",
        "outputId": "ea58994e-a517-497b-df3e-9aa93197dd0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial State: 0\n",
            "Environment Observation Space: Discrete(16)\n",
            "Environment Action Space: Discrete(4)\n",
            "[['S' 'F' 'F' 'F']\n",
            " ['F' 'H' 'F' 'H']\n",
            " ['F' 'F' 'F' 'H']\n",
            " ['H' 'F' 'F' 'G']]\n"
          ]
        }
      ],
      "source": [
        "# Print basic info about the environment\n",
        "print(\"Initial State:\", state)\n",
        "print(\"Environment Observation Space:\", env.observation_space)\n",
        "print(\"Environment Action Space:\", env.action_space)\n",
        "\n",
        "# Print the map layout\n",
        "print(env.unwrapped.desc.astype(str))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "module 'numpy' has no attribute 'bool8'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m     16\u001b[0m     action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()  \u001b[38;5;66;03m# Random action\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     state, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(terminated) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(truncated)  \u001b[38;5;66;03m# Explicit conversion to Python boolean\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     render_and_clear()\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\gym\\wrappers\\time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\gym\\wrappers\\order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\gym\\wrappers\\env_checker.py:37\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_step_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\gym\\utils\\passive_env_checker.py:233\u001b[0m, in \u001b[0;36menv_step_passive_checker\u001b[1;34m(env, action)\u001b[0m\n\u001b[0;32m    230\u001b[0m obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m result\n\u001b[0;32m    232\u001b[0m \u001b[38;5;66;03m# np.bool is actual python bool not np boolean type, therefore bool_ or bool8\u001b[39;00m\n\u001b[1;32m--> 233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(terminated, (\u001b[38;5;28mbool\u001b[39m, \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool8\u001b[49m)):\n\u001b[0;32m    234\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpects `terminated` signal to be a boolean, actual type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(terminated)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    236\u001b[0m     )\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncated, (\u001b[38;5;28mbool\u001b[39m, np\u001b[38;5;241m.\u001b[39mbool8)):\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\numpy\\__init__.py:427\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    424\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchar\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mchar\u001b[39;00m\n\u001b[0;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m char\u001b[38;5;241m.\u001b[39mchararray\n\u001b[1;32m--> 427\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    428\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;18m__name__\u001b[39m, attr))\n",
            "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'bool8'"
          ]
        }
      ],
      "source": [
        "from IPython.display import clear_output\n",
        "import time\n",
        "\n",
        "# Create environment with render_mode\n",
        "env = gym.make(\"FrozenLake-v1\", render_mode=\"human\")\n",
        "\n",
        "def render_and_clear():\n",
        "    clear_output(wait=True)\n",
        "    env.render()\n",
        "\n",
        "state, _ = env.reset()\n",
        "render_and_clear()\n",
        "\n",
        "done = False\n",
        "while not done:\n",
        "    action = env.action_space.sample()  # Random action\n",
        "    state, reward, terminated, truncated, _ = env.step(action)\n",
        "    done = bool(terminated) or bool(truncated)  # Explicit conversion to Python boolean\n",
        "    render_and_clear()\n",
        "    time.sleep(0.5)  # Add delay for better visibility"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU_2USY_lcKP"
      },
      "source": [
        "(b) Collect data from 10,000 episodes of agent interaction within the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzYRkEQ1lhV-",
        "outputId": "2bd7dee7-3795-451a-a189-e7f22e2e9f3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   State  Action  Reward  Goal_Proximity  Total_Reward\n",
            "0      0       2     0.0               6           0.0\n",
            "1      1       0     0.0               5           0.0\n",
            "2      0       3     0.0               6           0.0\n",
            "3      0       0     0.0               6           0.0\n",
            "4      4       3     0.0               5           0.0\n"
          ]
        }
      ],
      "source": [
        "# Initialize environment\n",
        "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True)\n",
        "goal_pos = (3, 3)  # Bottom-right corner (G)\n",
        "\n",
        "# Helper function to convert state to (row, col)\n",
        "def state_to_pos(state):\n",
        "    return divmod(state, 4)  # For 4x4 grid\n",
        "\n",
        "# Store data\n",
        "data = []\n",
        "\n",
        "n_episodes = 10000\n",
        "\n",
        "for episode in range(n_episodes):\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    episode_reward = 0\n",
        "    episode_data = []\n",
        "\n",
        "    while not done:\n",
        "        action = env.action_space.sample()  # Random policy\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        # Calculate goal proximity (Manhattan distance to (3, 3))\n",
        "        row, col = state_to_pos(state)\n",
        "        goal_distance = abs(goal_pos[0] - row) + abs(goal_pos[1] - col)\n",
        "\n",
        "        # Store current step\n",
        "        episode_data.append({\n",
        "            \"State\": state,\n",
        "            \"Action\": action,\n",
        "            \"Reward\": reward,\n",
        "            \"Goal_Proximity\": goal_distance\n",
        "        })\n",
        "\n",
        "        episode_reward += reward\n",
        "        state = next_state\n",
        "\n",
        "    # Add total reward to each step of the episode\n",
        "    for entry in episode_data:\n",
        "        entry[\"Total_Reward\"] = episode_reward\n",
        "        data.append(entry)\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Optional: Save to CSV\n",
        "df.to_csv(\"frozenlake_10000_episodes.csv\", index=False)\n",
        "\n",
        "# Show sample\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQQpAl5vMvki"
      },
      "source": [
        "(c) Calculate the proportion of episodes where the agent achieves the goal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KobG8lbXMtei",
        "outputId": "00c36f2a-8f3e-46ee-9eaf-c363d9954b0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent reached the goal in 128 out of 257 episodes.\n",
            "Proportion of success: 0.4981\n"
          ]
        }
      ],
      "source": [
        "# Get unique episodes by filtering at the end of each episode\n",
        "# Since Total_Reward is constant for all steps in an episode, we can group by episodes\n",
        "\n",
        "# Step 1: Split episodes by their ends using cumulative counter\n",
        "episode_counter = 0\n",
        "episode_rewards = []\n",
        "current_reward = None\n",
        "previous_state = None\n",
        "\n",
        "for i in range(len(df)):\n",
        "    if i == 0 or df.iloc[i][\"State\"] < df.iloc[i - 1][\"State\"]:  # naive check for episode restart\n",
        "        episode_counter += 1\n",
        "    current_reward = df.iloc[i][\"Total_Reward\"]\n",
        "    if i == len(df) - 1 or df.iloc[i + 1][\"Total_Reward\"] != current_reward:\n",
        "        episode_rewards.append(current_reward)\n",
        "\n",
        "# Step 2: Calculate proportion of success\n",
        "successes = sum(1 for r in episode_rewards if r == 1)\n",
        "total_episodes = len(episode_rewards)\n",
        "success_rate = successes / total_episodes\n",
        "\n",
        "print(f\"Agent reached the goal in {successes} out of {total_episodes} episodes.\")\n",
        "print(f\"Proportion of success: {success_rate:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnPcVN93PxwC"
      },
      "source": [
        " (d) Create a value representing the importance of action a taken at the given state s to achieve the final goal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGu7z_8KP3XS",
        "outputId": "583560cc-a664-45a9-99a6-0053cb7d8a58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   State  Action  Importance\n",
            "0      0       0    0.013204\n",
            "1      0       1    0.011589\n",
            "2      0       2    0.014120\n",
            "3      0       3    0.011787\n",
            "4      1       0    0.009214\n",
            "5      1       1    0.011994\n",
            "6      1       2    0.008207\n",
            "7      1       3    0.015335\n",
            "8      2       0    0.019648\n",
            "9      2       1    0.017655\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset if needed\n",
        "df = pd.read_csv(\"frozenlake_10000_episodes.csv\")\n",
        "\n",
        "# Group by State and Action, and calculate average Total_Reward as importance\n",
        "importance_df = df.groupby(['State', 'Action'])['Total_Reward'].mean().reset_index()\n",
        "\n",
        "importance_df.rename(columns={'Total_Reward': 'Importance'}, inplace=True)\n",
        "\n",
        "print(importance_df.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNVPTWNhRWEi"
      },
      "source": [
        "(e) Train a suitable machine learning model to predict the state-action value for a given state s and action a."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGNcqBg8RWU5",
        "outputId": "f7848b26-9bd3-4cfa-fa9b-657c79c72365"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training finished.\n",
            "Q-values for state 0: [0.19184586 0.19385462 0.01046148 0.08522756]\n",
            "Success rate over 1000 test episodes: 0.5240\n"
          ]
        }
      ],
      "source": [
        "# Create FrozenLake environment\n",
        "env = gym.make('FrozenLake-v1', is_slippery=True)\n",
        "\n",
        "# Initialize Q-table: states x actions\n",
        "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.8       # learning rate\n",
        "gamma = 0.95      # discount factor\n",
        "epsilon = 1.0     # exploration rate (epsilon-greedy)\n",
        "epsilon_decay = 0.9995\n",
        "epsilon_min = 0.01\n",
        "num_episodes = 10000\n",
        "max_steps_per_episode = 100\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "\n",
        "    for step in range(max_steps_per_episode):\n",
        "        # Choose action using epsilon-greedy policy\n",
        "        if np.random.random() < epsilon:\n",
        "            action = env.action_space.sample()  # Explore: random action\n",
        "        else:\n",
        "            action = np.argmax(Q[state])        # Exploit: best known action\n",
        "\n",
        "        # Take action and observe result\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        # Q-learning update rule\n",
        "        best_next_action = np.argmax(Q[next_state])\n",
        "        td_target = reward + gamma * Q[next_state][best_next_action]\n",
        "        td_error = td_target - Q[state][action]\n",
        "        Q[state][action] += alpha * td_error\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # Decay epsilon to reduce exploration over time\n",
        "    if epsilon > epsilon_min:\n",
        "        epsilon *= epsilon_decay\n",
        "\n",
        "# After training, Q contains the learned state-action values\n",
        "print(\"Training finished.\")\n",
        "\n",
        "# Example: print Q-values for state 0\n",
        "print(f\"Q-values for state 0: {Q[0]}\")\n",
        "\n",
        "# Optional: Evaluate the learned policy's success rate\n",
        "successes = 0\n",
        "test_episodes = 1000\n",
        "\n",
        "for _ in range(test_episodes):\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    for _ in range(max_steps_per_episode):\n",
        "        action = np.argmax(Q[state])  # Always pick best action\n",
        "        state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        if done:\n",
        "            successes += reward  # reward=1 if reached goal\n",
        "            break\n",
        "\n",
        "print(f\"Success rate over {test_episodes} test episodes: {successes / test_episodes:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mr_4AczRSCez"
      },
      "source": [
        "(f) Design and implement an algorithm to guide the agent to achieve the goal using the predictive model you have trained in part (e)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYonuAnHSCrp",
        "outputId": "8bc5b020-dec6-4224-a9ea-a1a6b96821eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode finished - Total Reward: 1.0, Steps Taken: 68, Success: True\n"
          ]
        }
      ],
      "source": [
        "def run_episode(env, Q, max_steps=100, render=False):\n",
        "    \"\"\"\n",
        "    Run one episode following the policy derived from Q-table.\n",
        "\n",
        "    Args:\n",
        "        env: Gym environment\n",
        "        Q: Trained Q-table (states x actions)\n",
        "        max_steps: Max steps before terminating\n",
        "        render: Whether to render the environment (visual)\n",
        "\n",
        "    Returns:\n",
        "        total_reward: cumulative reward received in the episode\n",
        "        steps_taken: number of steps before termination\n",
        "        success: boolean if goal was reached\n",
        "    \"\"\"\n",
        "    state, _ = env.reset()\n",
        "    total_reward = 0\n",
        "    steps_taken = 0\n",
        "    done = False\n",
        "\n",
        "    while not done and steps_taken < max_steps:\n",
        "        if render:\n",
        "            env.render()\n",
        "\n",
        "        # Select action greedily from Q-table\n",
        "        action = np.argmax(Q[state])\n",
        "\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        total_reward += reward\n",
        "        steps_taken += 1\n",
        "        state = next_state\n",
        "\n",
        "    success = total_reward > 0  # Reward = 1 if goal reached, else 0\n",
        "    return total_reward, steps_taken, success\n",
        "\n",
        "# Example usage:\n",
        "env = gym.make('FrozenLake-v1', is_slippery=True)\n",
        "\n",
        "total_reward, steps, success = run_episode(env, Q, render=False)\n",
        "print(f\"Episode finished - Total Reward: {total_reward}, Steps Taken: {steps}, Success: {success}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c3v-Sp5SO-4"
      },
      "source": [
        "(g) Evaluate the performance of your algorithm based on 10000 episodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7i7D8Y28SQIa",
        "outputId": "2747b0ea-7a45-4c70-8852-8341bc6b79c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10000 episodes:\n",
            "Success Rate: 0.5478\n",
            "Average Reward: 0.5478\n",
            "Average Steps Taken: 63.00\n"
          ]
        }
      ],
      "source": [
        "# Assume Q-table from part (e) is already trained and available as Q\n",
        "env = gym.make('FrozenLake-v1', is_slippery=True)\n",
        "\n",
        "def run_episode(env, Q, max_steps=100, render=False):\n",
        "    state, _ = env.reset()\n",
        "    total_reward = 0\n",
        "    steps_taken = 0\n",
        "    done = False\n",
        "\n",
        "    while not done and steps_taken < max_steps:\n",
        "        if render:\n",
        "            env.render()\n",
        "        action = np.argmax(Q[state])\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        total_reward += reward\n",
        "        steps_taken += 1\n",
        "        state = next_state\n",
        "\n",
        "    success = total_reward > 0\n",
        "    return total_reward, steps_taken, success\n",
        "\n",
        "# Evaluation over 10,000 episodes\n",
        "num_episodes = 10000\n",
        "success_count = 0\n",
        "total_rewards = 0\n",
        "total_steps = 0\n",
        "\n",
        "for _ in range(num_episodes):\n",
        "    reward, steps, success = run_episode(env, Q, render=False)\n",
        "    success_count += success\n",
        "    total_rewards += reward\n",
        "    total_steps += steps\n",
        "\n",
        "print(f\"Evaluation over {num_episodes} episodes:\")\n",
        "print(f\"Success Rate: {success_count / num_episodes:.4f}\")\n",
        "print(f\"Average Reward: {total_rewards / num_episodes:.4f}\")\n",
        "print(f\"Average Steps Taken: {total_steps / num_episodes:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdmYipr9ShgT"
      },
      "source": [
        " (h) Improve your learning algorithm by incorporating the concept of exploration–exploitation trade-off."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaCHTq0ZSlAB",
        "outputId": "a90d7b3a-f8a5-4f8d-bf18-d93ad9f52613"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Improved Q-learning with exploration-exploitation trade-off finished training.\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('FrozenLake-v1', is_slippery=True)\n",
        "\n",
        "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "\n",
        "alpha = 0.8\n",
        "gamma = 0.95\n",
        "epsilon = 1.0\n",
        "epsilon_min = 0.01\n",
        "epsilon_decay = 0.9998  # slower decay for better exploration\n",
        "num_episodes = 10000\n",
        "max_steps = 100\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        # Epsilon-greedy action selection\n",
        "        if np.random.random() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = np.argmax(Q[state])\n",
        "\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        best_next_action = np.argmax(Q[next_state])\n",
        "        td_target = reward + gamma * Q[next_state][best_next_action]\n",
        "        td_error = td_target - Q[state][action]\n",
        "        Q[state][action] += alpha * td_error\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # Decay epsilon but not below minimum threshold\n",
        "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "print(\"Improved Q-learning with exploration-exploitation trade-off finished training.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SumWEVODS4ug"
      },
      "source": [
        "(i) Evaluate the performance of your improved learning algorithm based on 10000\n",
        " episodes and compare it with the algorithm implemented in part (f)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F84_QJD8S5xx",
        "outputId": "fd661562-3bf5-4681-abba-bec78d357586"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Q-learning policy over 10,000 episodes:\n",
            "Success rate: 0.0000, Avg reward: 0.0000, Avg steps: 18.13\n",
            "Improved Q-learning policy over 10,000 episodes:\n",
            "Success rate: 0.3008, Avg reward: 0.3008, Avg steps: 31.03\n",
            "\n",
            "Comparison:\n",
            "Success rate improvement: 0.3008\n",
            "Average reward improvement: 0.3008\n",
            "Average steps difference: 12.90\n"
          ]
        }
      ],
      "source": [
        "def run_episode(env, Q, max_steps=100, render=False):\n",
        "    state, _ = env.reset()\n",
        "    total_reward = 0\n",
        "    steps_taken = 0\n",
        "    done = False\n",
        "\n",
        "    while not done and steps_taken < max_steps:\n",
        "        if render:\n",
        "            env.render()\n",
        "        action = np.argmax(Q[state])\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        total_reward += reward\n",
        "        steps_taken += 1\n",
        "        state = next_state\n",
        "\n",
        "    success = total_reward > 0\n",
        "    return total_reward, steps_taken, success\n",
        "\n",
        "\n",
        "# Environment\n",
        "env = gym.make('FrozenLake-v1', is_slippery=True)\n",
        "\n",
        "# --- Original Q-learning policy (part f) ---\n",
        "# Assume Q_original is the Q-table you got from basic Q-learning without exploration decay\n",
        "# For demonstration, suppose Q_original is already available (replace with your Q)\n",
        "Q_original = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "# ... (load or train Q_original accordingly) ...\n",
        "\n",
        "# --- Improved Q-learning with epsilon decay (part h) ---\n",
        "# Train improved Q (already done)\n",
        "Q_improved = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "alpha = 0.8\n",
        "gamma = 0.95\n",
        "epsilon = 1.0\n",
        "epsilon_min = 0.01\n",
        "epsilon_decay = 0.9998\n",
        "num_training_episodes = 10000\n",
        "max_steps = 100\n",
        "\n",
        "for episode in range(num_training_episodes):\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    for step in range(max_steps):\n",
        "        if np.random.random() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = np.argmax(Q_improved[state])\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        best_next_action = np.argmax(Q_improved[next_state])\n",
        "        td_target = reward + gamma * Q_improved[next_state][best_next_action]\n",
        "        td_error = td_target - Q_improved[state][action]\n",
        "        Q_improved[state][action] += alpha * td_error\n",
        "\n",
        "        state = next_state\n",
        "        if done:\n",
        "            break\n",
        "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "# --- Evaluation function ---\n",
        "def evaluate_policy(env, Q, episodes=10000):\n",
        "    success_count = 0\n",
        "    total_rewards = 0\n",
        "    total_steps = 0\n",
        "    for _ in range(episodes):\n",
        "        reward, steps, success = run_episode(env, Q)\n",
        "        success_count += success\n",
        "        total_rewards += reward\n",
        "        total_steps += steps\n",
        "    return success_count / episodes, total_rewards / episodes, total_steps / episodes\n",
        "\n",
        "\n",
        "# Evaluate original policy\n",
        "success_orig, avg_reward_orig, avg_steps_orig = evaluate_policy(env, Q_original)\n",
        "print(f\"Original Q-learning policy over 10,000 episodes:\")\n",
        "print(f\"Success rate: {success_orig:.4f}, Avg reward: {avg_reward_orig:.4f}, Avg steps: {avg_steps_orig:.2f}\")\n",
        "\n",
        "# Evaluate improved policy\n",
        "success_impr, avg_reward_impr, avg_steps_impr = evaluate_policy(env, Q_improved)\n",
        "print(f\"Improved Q-learning policy over 10,000 episodes:\")\n",
        "print(f\"Success rate: {success_impr:.4f}, Avg reward: {avg_reward_impr:.4f}, Avg steps: {avg_steps_impr:.2f}\")\n",
        "\n",
        "# Compare\n",
        "print(\"\\nComparison:\")\n",
        "print(f\"Success rate improvement: {success_impr - success_orig:.4f}\")\n",
        "print(f\"Average reward improvement: {avg_reward_impr - avg_reward_orig:.4f}\")\n",
        "print(f\"Average steps difference: {avg_steps_impr - avg_steps_orig:.2f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
